## 概念

指在计算机执行操作时，CPU 不需要先将数据从一个内存区域复制到另一个内存区域，从而可以**减少上下文切换以及 CPU 的拷贝时间**。它的作用是在数据报从网络设备到用户程序空间传递的过程中，减少数据拷贝次数，减少系统调用，实现 CPU 的零参与，彻底消除 CPU 在这方面的负载。实现零拷贝用到的最主要技术是 **DMA 数据传输技术**和**内存区域映射技术**。

## Linux 的 IO 方式

Linux 提供了**轮询、I/O 中断以及 DMA 传输**这 3 种磁盘与主存之间的数据传输机制。

- 轮询方式是基于死循环对 I/O 端口进行不断检测。
- I/O 中断方式是指当数据到达时，磁盘主动向 CPU 发起中断请求，由 CPU 自身负责数据的传输过程。
- DMA 传输则在 I/O 中断的基础上引入了 DMA 磁盘控制器，由 DMA 磁盘控制器负责数据的传输，降低了 I/O 中断操作对 CPU 资源的大量消耗。

### I/O 中断方式

### DMA 传输原理

## 零拷贝方式

在 Linux 中零拷贝技术主要有 3 个实现思路：**用户态直接 I/O、减少数据拷贝次数以及写时复制技术**。

### 用户态直接 I/O

用户直接 I/O 使得数据直接跨过内核传输，应用进程或运行在用户态下的库函数直接访问硬件设备。

![](https://community-header-1306990603.cos.ap-guangzhou.myqcloud.com/20220119132317.png)

用户态直接 I/O 只能适用于不需要内核缓冲区处理的应用程序，这些应用程序通常在进程地址空间有自己的数据缓存机制，称为自缓存应用程序，如数据库管理系统就是一个代表。其次，这种零拷贝机制会直接操作磁盘 I/O，由于 CPU 和磁盘 I/O 之间的执行时间差距，会造成大量资源的浪费，解决方案是配合异步 I/O 使用。

### mmap+write

使用 mmap + write 代替原来的 read + write 方式，**减少了 1 次 CPU 拷贝操作**。mmap 是 Linux 提供的一种内存映射文件方法，即将一个进程的地址空间中的一段虚拟地址映射到磁盘文件地址。

mmap 将内核中读缓冲区域与用户缓冲区进行映射，从而实现内核缓冲区和应用程序内存的共享，省去将数据从内核读缓冲区拷贝到用户缓冲区的过程，如图所示

![](https://community-header-1306990603.cos.ap-guangzhou.myqcloud.com/20220119132642.png)

mmap 主要的用处是**提高 I/O 性能**，特别是针对大文件。对**于小文件，内存映射文件反而会导致碎片空间的浪费**，因为内存映射总是要对齐页边界，最小单位是 4 KB，一个 5 KB 的文件将会映射占用 8 KB 内存，也就会浪费 3 KB 内存。

**缺点**：当 mmap 一个文件时，如果这个文件被另一个进程所截获，那么 write 系统调用会因为访问非法地址被 SIGBUS 信号终止，SIGBUS 默认会杀死进程并产生一个 coredump，服务器可能因此被终止。

### sendfile

通过 sendfile 系统调用，数据可以直接在内核空间内部进行 I/O 传输，从而省去了数据在用户空间和内核空间之间的来回拷贝。与 mmap 内存映射方式不同的是， sendfile 调用中 I/O 数据对用户空间是完全不可见的。也就是说，这是一次完全意义上的数据传输过程。

![](https://community-header-1306990603.cos.ap-guangzhou.myqcloud.com/20220119134347.png)

基于 sendfile 系统调用的零拷贝方式，整个拷贝过程会发生 2 次上下文切换，1 次 CPU 拷贝和 2 次 DMA 拷贝，用户程序读写数据的流程如下：

1. 用户进程通过 sendfile() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。
2. DMA 控制器将数据从主存或硬盘拷贝到内核空间（kernel space）的读缓冲区（read buffer）。
3. CPU 将读缓冲区（read buffer）中的数据拷贝到的网络缓冲区（socket buffer）。
4. DMA 控制器将数据从网络缓冲区（socket buffer）拷贝到网卡进行数据传输。
5. 上下文从内核态（kernel space）切换回用户态（user space），sendfile 系统调用执行返回。

相比较于 mmap 内存映射的方式，sendfile 少了 2 次上下文切换，但是**仍然有 1 次 CPU 拷贝操作**(第三步)。sendfile 存在的问题是用户程序不能对数据进行修改，而只是单纯地完成了一次数据传输过程。

### sendfile + DMA gather copy

Linux2.4 版本内核 sendfile 系统调用进行修改，为 DMA 拷贝引入 gather 操作。它将内核空间的读缓冲区中对应的数据描述信息（内存地址、地址偏移量）记录到相应的网络缓冲区中，**由 DMA 根据内存地址、地址偏移量将数据批量地从读缓冲区拷贝到网卡设备中，这样就省去了内核空间中仅剩的 1 次 CPU 拷贝操作**

![](https://community-header-1306990603.cos.ap-guangzhou.myqcloud.com/20220119135042.png)

基于 sendfile + DMA gather copy 系统调用的零拷贝方式，整个拷贝过程会发生 2 次上下文切换、0 次 CPU 拷贝以及 2 次 DMA 拷贝，用户程序读写数据的流程如下：

1. 用户进程通过 sendfile() 函数向内核（kernel）发起系统调用，上下文从用户态（user space）切换为内核态（kernel space）。
2. CPU 利用 DMA 控制器将数据从主存或硬盘拷贝到内核空间（kernel space）的读缓冲区（read buffer）。
3. CPU 把读缓冲区（read buffer）的文件描述符（file descriptor）和数据长度拷贝到网络缓冲区（socket buffer）。
4. 基于已拷贝的文件描述符（file descriptor）和数据长度，CPU 利用 DMA 控制器的 gather/scatter 操作直接批量地将数据从内核的读缓冲区（read buffer）拷贝到网卡进行数据传输。
5. 上下文从内核态（kernel space）切换回用户态（user space），sendfile 系统调用执行返回。

sendfile + DMA gather copy 拷贝方式同样存在用户程序不能对数据进行修改的问题，而且本身需要硬件的支持，它**只适用于将数据从文件拷贝到 socket 套接字上的传输过程。**
### splice
Linux在2.6.17引入splice系统调用，无需硬件支持，实现了两个文件描述符之间的数据零拷贝。

splice 系统调用可以在内核空间的读缓冲区（read buffer）和网络缓冲区（socket buffer）之间建立管道（pipeline），从而避免了两者之间的 CPU 拷贝操作。

![](https://community-header-1306990603.cos.ap-guangzhou.myqcloud.com/20220119144500.png)

splice 拷贝方式也同样存在用户程序不能对数据进行修改的问题。除此之外，它使用了 Linux 的**管道缓冲机制**，可以用于任意两个文件描述符中传输数据，但是它的两个文件描述符参数中有一个必须是管道设备。

### 写时复制
在某些情况下，内核缓冲区可能被多个进程所共享，如果某个进程想要这个共享区进行 write 操作，由于 write 不提供任何的锁操作，那么就会对共享区中的数据造成破坏，写时复制的引入就是 Linux 用来保护数据的。

写时复制指的是当多个进程共享同一块数据时，如果其中一个进程需要对这份数据进行修改，那么就需要将其拷贝到自己的进程地址空间中。这样做并不影响其他进程对这块数据的操作，每个进程要修改的时候才会进行拷贝，所以叫写时拷贝。这种方法在某种程度上能够降低系统开销，如果某个进程永远不会对所访问的数据进行更改，那么也就永远不需要拷贝。
[]()

### Linux 零拷贝对比

|         拷贝方式         | CPU 拷贝 | DMA 拷贝 |  系统调用  | 上下文切换 |
| :----------------------: | :------: | :------: | :--------: | :--------: |
|   传统方式(read+write)   |    2     |    2     | read/write |     4      |
|   内存映射(mmap+write)   |    1     |    2     | mmap/write |     4      |
|         sendfile         |    1     |    2     |  sendfile  |     2      |
| sendfile+DMA gather copy |    0     |    2     |  sendfile  |     2      |
|          splice          |    0     |    2     |   splice   |     2      |

## Java NIO零拷贝实现(待补充)
在 Java NIO 中的通道（Channel）就相当于操作系统的内核空间（kernel space）的缓冲区，而缓冲区（Buffer）对应的相当于操作系统的用户空间（user space）中的用户缓冲区（user buffer）。

* 通道（Channel）是全双工的（双向传输），它既可能是读缓冲区（read buffer），也可能是网络缓冲区（socket buffer）。
* 缓冲区（Buffer）分为堆内存（HeapBuffer）和堆外内存（DirectBuffer），这是通过 malloc() 分配出来的用户态内存。

1. **MappedByteBuffer**：

MappedByteBuffer 是 NIO 基于内存映射（mmap）这种零拷贝方式的提供的一种实现，它继承自 ByteBuffer。

2. **DirectByteBuffer**：

DirectByteBuffer 的对象引用位于 Java 内存模型的堆里面，JVM 可以对DirectByteBuffer 的对象进行内存分配和回收管理，一般使用 DirectByteBuffer 的静态方法 allocateDirect() 创建 DirectByteBuffer 实例并分配内存。

## 零拷贝机制的应用
### Netty
Netty对零拷贝有三个层次的实现

1. **避免数据流经用户空间**
   
    Netty 通过 DefaultFileRegion 类对 java.nio.channels.FileChannel 的 tranferTo() 方法进行包装，在文件传输时可以将文件缓冲区的数据直接发送到目的通道（Channel），即从用户空间程序buffer到内核空间socket写缓冲的数据流动。这是属于操作系统级别的零拷贝。

2. **避免数据从JVM Heap到C Heap的拷贝**
   
    在JVM层面，每当程序需要执行一个I/O操作时，都需要**将数据先从JVM管理的堆内存复制到使用C malloc()或类似函数分配的Heap内存中才能够触发系统调用完成操作，这部分内存站在Java程序的视角来看就是堆外内存**

3. **减少数据在用户空间的多次拷贝**

    对于ByteBuffer，常常需要进行数据读取到自己开辟的一处byte[]中遍历处理，或者组合多个ByteBuffer到一个更大的byte[]数组，导致数据的复制。

   Netty提供了`CompositeByteBuf`类，将多个ByteBuffer逻辑上当成一个完整的ByteBuffer操作，免去了重新分配空间再复制数据的开销。

   而ByteBuf可以通过 wrap 操作把字节数组、ByteBuf、ByteBuffer 包装成一个 ByteBuf 对象, 进而避免了拷贝操作，也支持slice 操作, 因此可以将 ByteBuf 分解为多个共享同一个存储区域的 ByteBuf，避免了内存的拷贝
### Kafka
Kafka的数据文件采用sendfile的方式，而索引文件采用mmap+write的方式。
## 参考

https://zhuanlan.zhihu.com/p/83398714
